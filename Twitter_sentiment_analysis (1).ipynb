{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw1dwyM06KB7",
        "outputId": "f38b7ce9-44a0-45ff-a49d-3e06770153e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "import string\n",
        "from nltk import pos_tag\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('twitter_x_y_train.csv')"
      ],
      "metadata": {
        "id": "pbznnnLI8woy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = list(data['airline_sentiment'])"
      ],
      "metadata": {
        "id": "8Usyyvo49LkU"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_simple_pos(tag): #creating simple tags to pass into the lemmatizer\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN"
      ],
      "metadata": {
        "id": "gupNTB2rURWA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "def clean_tweet(tweet):\n",
        "    words = word_tokenize(tweet)\n",
        "    output_words = []\n",
        "    for w in words:\n",
        "        if w.lower() not in list(string.punctuation):\n",
        "            pos = pos_tag([w])\n",
        "            clean_word = lemmatizer.lemmatize(w, get_simple_pos(pos[0][1]))\n",
        "            output_words.append(clean_word.lower())\n",
        "    return \" \".join(output_words)"
      ],
      "metadata": {
        "id": "m4_T_d62UXka"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = []\n",
        "for twt in data['text']:\n",
        "    x_data.append(clean_tweet(twt))"
      ],
      "metadata": {
        "id": "eNQAWCaUUbR-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#x_train, x_test, y_train, y_test = train_test_split(x_data, y, random_state= 1)"
      ],
      "metadata": {
        "id": "NY8v8CEvZzv1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer(max_features= 5000, ngram_range=(1, 2), max_df = 0.7)"
      ],
      "metadata": {
        "id": "brSWszK3Mwh6"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_vect = vectorizer.fit_transform(x_data)\n",
        "#x_test_vect = vectorizer.transform(x_test)"
      ],
      "metadata": {
        "id": "eUXaRQgC97rg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = pd.read_csv('/content/twitter_x_test.csv')\n",
        "x_test_data = []\n",
        "for twt in x_test['text']:\n",
        "    x_test_data.append(clean_tweet(twt))\n",
        "x_test_vect = vectorizer.transform(x_test_data)"
      ],
      "metadata": {
        "id": "O9EMxtX2Hzbd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "svc = svm.SVC()\n",
        "svc.fit(x_train_vect, y)\n",
        "#svc.score(x_test_vect, y_test)\n",
        "y_pred = svc.predict(x_test_vect)"
      ],
      "metadata": {
        "id": "j9Qc-YAA-N5S"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd_series = pd.Series(y_pred)\n",
        "pd_series.to_csv('predicted_sentiments.csv', index = False, header= False)"
      ],
      "metadata": {
        "id": "Rrw7I4Nv_FaB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtUyYdosJvD7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}